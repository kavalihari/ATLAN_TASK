{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "id": "4iIP3rbRgOkT"
      },
      "id": "4iIP3rbRgOkT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow\n",
        "!pip install matplotlib\n",
        "!pip install opencv-python"
      ],
      "metadata": {
        "id": "QgnkCTcLgVHh"
      },
      "id": "QgnkCTcLgVHh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
        "\n",
        "model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
        "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")"
      ],
      "metadata": {
        "id": "mebZgJGQgOg1"
      },
      "id": "mebZgJGQgOg1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import skimage\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Download sample image\n",
        "image = skimage.data.astronaut()\n",
        "image = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n",
        "\n",
        "# Text queries to search the image for\n",
        "text_queries = [\"human face\", \"rocket\", \"nasa badge\", \"star-spangled banner\"]\n",
        "\n",
        "image"
      ],
      "metadata": {
        "id": "5v12ZSo9gOdB"
      },
      "id": "5v12ZSo9gOdB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Use GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "iftp3ZG2gOax"
      },
      "id": "iftp3ZG2gOax",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(text=text_queries, images=image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Print input names and shapes\n",
        "for key, val in inputs.items():\n",
        "    print(f\"{key}: {val.shape}\")"
      ],
      "metadata": {
        "id": "3BboukfcgOYl"
      },
      "id": "3BboukfcgOYl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set model in evaluation mode\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Get predictions\n",
        "with torch.no_grad():\n",
        "  outputs = model(**inputs)\n",
        "\n",
        "for k, val in outputs.items():\n",
        "    if k not in {\"text_model_output\", \"vision_model_output\"}:\n",
        "        print(f\"{k}: shape of {val.shape}\")\n",
        "\n",
        "print(\"\\nText model outputs\")\n",
        "for k, val in outputs.text_model_output.items():\n",
        "    print(f\"{k}: shape of {val.shape}\")\n",
        "\n",
        "print(\"\\nVision model outputs\")\n",
        "for k, val in outputs.vision_model_output.items():\n",
        "    print(f\"{k}: shape of {val.shape}\")"
      ],
      "metadata": {
        "id": "irAYqd7VgOV2"
      },
      "id": "irAYqd7VgOV2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from transformers.image_utils import ImageFeatureExtractionMixin\n",
        "mixin = ImageFeatureExtractionMixin()\n",
        "\n",
        "# Load example image\n",
        "image_size = model.config.vision_config.image_size\n",
        "image = mixin.resize(image, image_size)\n",
        "input_image = np.asarray(image).astype(np.float32) / 255.0\n",
        "\n",
        "# Threshold to eliminate low probability predictions\n",
        "score_threshold = 0.1\n",
        "\n",
        "# Get prediction logits\n",
        "logits = torch.max(outputs[\"logits\"][0], dim=-1)\n",
        "scores = torch.sigmoid(logits.values).cpu().detach().numpy()\n",
        "\n",
        "# Get prediction labels and boundary boxes\n",
        "labels = logits.indices.cpu().detach().numpy()\n",
        "boxes = outputs[\"pred_boxes\"][0].cpu().detach().numpy()"
      ],
      "metadata": {
        "id": "3RL4c3T_gORx"
      },
      "id": "3RL4c3T_gORx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(input_image, text_queries, scores, boxes, labels):\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "    ax.imshow(input_image, extent=(0, 1, 1, 0))\n",
        "    ax.set_axis_off()\n",
        "\n",
        "    for score, box, label in zip(scores, boxes, labels):\n",
        "      if score < score_threshold:\n",
        "        continue\n",
        "\n",
        "      cx, cy, w, h = box\n",
        "      ax.plot([cx-w/2, cx+w/2, cx+w/2, cx-w/2, cx-w/2],\n",
        "              [cy-h/2, cy-h/2, cy+h/2, cy+h/2, cy-h/2], \"r\")\n",
        "      ax.text(\n",
        "          cx - w / 2,\n",
        "          cy + h / 2 + 0.015,\n",
        "          f\"{text_queries[label]}: {score:1.2f}\",\n",
        "          ha=\"left\",\n",
        "          va=\"top\",\n",
        "          color=\"red\",\n",
        "          bbox={\n",
        "              \"facecolor\": \"white\",\n",
        "              \"edgecolor\": \"red\",\n",
        "              \"boxstyle\": \"square,pad=.3\"\n",
        "          })\n",
        "\n",
        "plot_predictions(input_image, text_queries, scores, boxes, labels)"
      ],
      "metadata": {
        "id": "0nerW5uDgOPv"
      },
      "id": "0nerW5uDgOPv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the coffee mug image\n",
        "image = skimage.data.coffee()\n",
        "image = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n",
        "image"
      ],
      "metadata": {
        "id": "7btcpkocgOL0"
      },
      "id": "7btcpkocgOL0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "images = [skimage.data.astronaut(), skimage.data.coffee()]\n",
        "images = [Image.fromarray(np.uint8(img)).convert(\"RGB\") for img in images]\n",
        "\n",
        "# Nexted list of text queries to search each image for\n",
        "text_queries = [[\"human face\", \"rocket\", \"nasa badge\", \"star-spangled banner\"], [\"coffee mug\", \"spoon\", \"plate\"]]\n",
        "\n",
        "# Process image and text inputs\n",
        "inputs = processor(text=text_queries, images=images, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Print input names and shapes\n",
        "for key, val in inputs.items():\n",
        "    print(f\"{key}: {val.shape}\")"
      ],
      "metadata": {
        "id": "sJAWTKR6hTJs"
      },
      "id": "sJAWTKR6hTJs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions\n",
        "with torch.no_grad():\n",
        "  outputs = model(**inputs)\n",
        "\n",
        "for k, val in outputs.items():\n",
        "    if k not in {\"text_model_output\", \"vision_model_output\"}:\n",
        "        print(f\"{k}: shape of {val.shape}\")\n",
        "\n",
        "print(\"\\nText model outputs\")\n",
        "for k, val in outputs.text_model_output.items():\n",
        "    print(f\"{k}: shape of {val.shape}\")\n",
        "\n",
        "print(\"\\nVision model outputs\")\n",
        "for k, val in outputs.vision_model_output.items():\n",
        "    print(f\"{k}: shape of {val.shape}\")"
      ],
      "metadata": {
        "id": "MRBxHWP9hTBc"
      },
      "id": "MRBxHWP9hTBc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's plot the predictions for the second image\n",
        "image_idx = 1\n",
        "image_size = model.config.vision_config.image_size\n",
        "image = mixin.resize(images[image_idx], image_size)\n",
        "input_image = np.asarray(image).astype(np.float32) / 255.0\n",
        "\n",
        "# Threshold to eliminate low probability predictions\n",
        "score_threshold = 0.1\n",
        "\n",
        "# Get prediction logits\n",
        "logits = torch.max(outputs[\"logits\"][image_idx], dim=-1)\n",
        "scores = torch.sigmoid(logits.values).cpu().detach().numpy()\n",
        "\n",
        "# Get prediction labels and boundary boxes\n",
        "labels = logits.indices.cpu().detach().numpy()\n",
        "boxes = outputs[\"pred_boxes\"][image_idx].cpu().detach().numpy()\n",
        "\n",
        "plot_predictions(input_image, text_queries[image_idx], scores, boxes, labels)"
      ],
      "metadata": {
        "id": "TO_ZB6p1hS69"
      },
      "id": "TO_ZB6p1hS69",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n",
        "target_sizes = torch.Tensor([img.size[::-1] for img in images]).to(device)\n",
        "\n",
        "# Convert outputs (bounding boxes and class logits) to COCO API\n",
        "results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\n",
        "\n",
        "# Loop over predictions for each image in the batch\n",
        "for i in range(len(images)):\n",
        "    print(f\"\\nProcessing image {i}\")\n",
        "    text = text_queries[i]\n",
        "    boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n",
        "\n",
        "    score_threshold = 0.1\n",
        "    for box, score, label in zip(boxes, scores, labels):\n",
        "        box = [round(i, 2) for i in box.tolist()]\n",
        "\n",
        "        if score >= score_threshold:\n",
        "            print(f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\")"
      ],
      "metadata": {
        "id": "MHbuUIGXhS0T"
      },
      "id": "MHbuUIGXhS0T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import requests\n",
        "from matplotlib import rcParams\n",
        "\n",
        "# Set figure size\n",
        "%matplotlib inline\n",
        "rcParams['figure.figsize'] = 11 ,8\n",
        "\n",
        "# Input image\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "target_sizes = torch.Tensor([image.size[::-1]])\n",
        "\n",
        "# Query image\n",
        "query_url = \"http://images.cocodataset.org/val2017/000000058111.jpg\"\n",
        "query_image = Image.open(requests.get(query_url, stream=True).raw)\n",
        "\n",
        "# Display input image and query image\n",
        "fig, ax = plt.subplots(1,2)\n",
        "ax[0].imshow(image)\n",
        "ax[1].imshow(query_image)"
      ],
      "metadata": {
        "id": "GAkoIuqBhStW"
      },
      "id": "GAkoIuqBhStW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process input and query image\n",
        "inputs = processor(images=image, query_images=query_image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Print input names and shapes\n",
        "for key, val in inputs.items():\n",
        "    print(f\"{key}: {val.shape}\")"
      ],
      "metadata": {
        "id": "KG0iyUFMhSjv"
      },
      "id": "KG0iyUFMhSjv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions\n",
        "with torch.no_grad():\n",
        "  outputs = model.image_guided_detection(**inputs)\n",
        "\n",
        "for k, val in outputs.items():\n",
        "    if k not in {\"text_model_output\", \"vision_model_output\"}:\n",
        "        print(f\"{k}: shape of {val.shape}\")\n",
        "\n",
        "print(\"\\nVision model outputs\")\n",
        "for k, val in outputs.vision_model_output.items():\n",
        "    print(f\"{k}: shape of {val.shape}\")"
      ],
      "metadata": {
        "id": "XBSFv4dxhSbC"
      },
      "id": "XBSFv4dxhSbC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2RGB)\n",
        "outputs.logits = outputs.logits.cpu()\n",
        "outputs.target_pred_boxes = outputs.target_pred_boxes.cpu()\n",
        "\n",
        "results = processor.post_process_image_guided_detection(outputs=outputs, threshold=0.6, nms_threshold=0.3, target_sizes=target_sizes)\n",
        "boxes, scores = results[0][\"boxes\"], results[0][\"scores\"]\n",
        "\n",
        "# Draw predicted bounding boxes\n",
        "for box, score in zip(boxes, scores):\n",
        "    box = [int(i) for i in box.tolist()]\n",
        "\n",
        "    img = cv2.rectangle(img, box[:2], box[2:], (255,0,0), 5)\n",
        "    if box[3] + 25 > 768:\n",
        "        y = box[3] - 10\n",
        "    else:\n",
        "        y = box[3] + 25\n",
        "\n",
        "plt.imshow(img[:,:,::-1])"
      ],
      "metadata": {
        "id": "loLKlc_IhSTO"
      },
      "id": "loLKlc_IhSTO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7dvtzAE1hSNa"
      },
      "id": "7dvtzAE1hSNa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cx3HlZPihSHm"
      },
      "id": "Cx3HlZPihSHm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YTzdJFWEhSB2"
      },
      "id": "YTzdJFWEhSB2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZtmT92RzhR86"
      },
      "id": "ZtmT92RzhR86",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dFhx0yTNhR3a"
      },
      "id": "dFhx0yTNhR3a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bKjMlzxkhRyc"
      },
      "id": "bKjMlzxkhRyc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UVXVSVqnhRtS"
      },
      "id": "UVXVSVqnhRtS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m3So6mX9hRoS"
      },
      "id": "m3So6mX9hRoS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "llQ3iVHnhRiS"
      },
      "id": "llQ3iVHnhRiS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ff7eafe4",
      "metadata": {
        "id": "ff7eafe4"
      },
      "source": [
        "\n",
        "# LeRobot Training Instruction Manual\n",
        "\n",
        "# ðŸŽ‰ Welcome to the LeRobot Training Notebook!\n",
        "\n",
        "This guide will help you set up and train a model on a cloud-based platform, such as **Google Colab**, using **LeRobot** with **Hugging Face**.\n",
        "\n",
        "---\n",
        "\n",
        "## âš ï¸ **Disclaimers:**\n",
        "\n",
        "- **GPU Subscription**: ðŸ”‘ Make sure you have the appropriate subscription plan that provides access to the necessary GPU (e.g., **A100**, **T4**). Review pricing and benefits on the cloud provider's website before proceeding.\n",
        "  \n",
        "- **Checkpoint Requirement**: â³ If resuming training, ensure that you have the previous training checkpoint available in your session. Without the checkpoint, the training **cannot be resumed**.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ **Important Instructions:**\n",
        "\n",
        "- **Run All Cells Together**: ðŸ”„ It is recommended to run all the cells in one go if you plan to leave the session **unmonitored**. This helps avoid session timeouts or disruptions.\n",
        "\n",
        "- **GPU & Compute Units**: ðŸŽ›ï¸ Ensure you select a suitable GPU (e.g., **A100**, **T4**) and have enough compute units for your session. A typical 5-hour training session requires approximately **70 compute units**.\n",
        "\n",
        "- **Monitor Training**: ðŸ‘€ Itâ€™s advisable to monitor the **first few epochs** to ensure that the training is running smoothly before leaving the session unattended.\n",
        "\n",
        "- **Local Storage**: ðŸ’¾ You will be prompted to choose whether you want to store the training outputs **locally** at the end of the process.\n",
        "\n",
        "---\n",
        "\n",
        "Now, letâ€™s begin the setup process! ðŸš€\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e65a27ba",
      "metadata": {
        "id": "e65a27ba"
      },
      "outputs": [],
      "source": [
        "# Collect all necessary inputs from the user\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# GPU selection (Reminder: Ensure enough compute units for smooth training)\n",
        "print(\"Please select a suitable GPU type (e.g., A100, T4) for cloud-based training.\")\n",
        "\n",
        "# Hugging Face login token\n",
        "print(\"Generate a Hugging Face token from: https://huggingface.co/settings/tokens\")\n",
        "hf_token = input(\"Please enter your Hugging Face token: \")\n",
        "\n",
        "# Link to Trossen Robotics Community datasets\n",
        "print(\"You can explore datasets from Trossen Robotics Community here: https://huggingface.co/TrossenRoboticsCommunity\")\n",
        "\n",
        "# Dataset and job details\n",
        "dataset_repo_id = input(\"Please enter the dataset repo ID from Hugging Face (e.g., TrossenRoboticsCommunity/aloha_stationary_logo_assembly): \")\n",
        "\n",
        "\n",
        "print(\"**Important**: Use a valid directory/jobs name. Avoid numbers or special characters other than '_'.\")\n",
        "print(\"Example: 'training_results_aloha' or 'aloha_training_output'\")\n",
        "\n",
        "job_name = input(\"Please enter the job name for this training session: \")\n",
        "\n",
        "# Output directory with naming format instructions\n",
        "\n",
        "output_dir = input(\"Enter the output directory name: \")\n",
        "\n",
        "# Resume flag with disclaimer\n",
        "resume_flag = input(\"Do you want to resume training from a previous checkpoint? (yes/no): \")\n",
        "resume_cmd = \"--resume\" if resume_flag.lower() == 'yes' else \"\"\n",
        "\n",
        "# Model upload flag\n",
        "upload_choice = input(\"Do you want to upload the model to Hugging Face after training? (yes/no): \")\n",
        "model_repo_id = \"\"\n",
        "if upload_choice.lower() == 'yes':\n",
        "    model_repo_id = input(\"Please enter the model repo ID to store your trained model to Hugging Face (e.g., TrossenRoboticsCommunity/aloha_stationary_logo_assembly): \")\n",
        "\n",
        "# Local storage flag\n",
        "store_locally = input(\"Do you want to store the training outputs locally? (yes/no): \")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6350b26b",
      "metadata": {
        "id": "6350b26b"
      },
      "source": [
        "\n",
        "## Step 1: GPU Setup & Compute Units\n",
        "\n",
        "The GPU type you selected earlier will now be configured for this cloud-based training session. Make sure to have enough compute units to support long training sessions, and monitor the first few epochs to ensure smooth execution.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46a58617",
      "metadata": {
        "id": "46a58617"
      },
      "source": [
        "\n",
        "## Step 2: Installing Dependencies\n",
        "\n",
        "In this step, we'll install all the necessary dependencies for running LeRobot and performing model training.\n",
        "\n",
        "Ensure that these packages are successfully installed before proceeding to the next steps.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49888a14",
      "metadata": {
        "id": "49888a14"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Install required dependencies\n",
        "print(\"Installing required dependencies...\")\n",
        "\n",
        "!sudo apt-get install libusb-1.0-0-dev\n",
        "!pip install --upgrade  pyrealsense2 dynamixel-sdk rerun-sdk blinker wandb datasets huggingface-hub hydra-core gitpython flask diffusers InquirerPy\n",
        "\n",
        "# Install blinker if needed\n",
        "!pip install --ignore-installed blinker\n",
        "\n",
        "\n",
        "# Install LeRobot repository\n",
        "!git clone https://github.com/huggingface/lerobot.git\n",
        "%cd /content/lerobot\n",
        "!ls\n",
        "!pip install -e .\n",
        "!pip install .[intelrealsense,dynamixel]\n",
        "\n",
        "print(\"Dependencies installed successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9000e90a",
      "metadata": {
        "id": "9000e90a"
      },
      "source": [
        "\n",
        "## Step 3: Hugging Face Login & Dataset Setup\n",
        "\n",
        "We will now log into Hugging Face using the token provided. After login, the dataset repo, job name, and output directory that you specified will be configured for the training session.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5f45843",
      "metadata": {
        "id": "e5f45843"
      },
      "outputs": [],
      "source": [
        "# Log in to Hugging Face and verify login\n",
        "print(\"Logging into Hugging Face...\")\n",
        "!huggingface-cli login --token {hf_token}\n",
        "\n",
        "# Verify the login by checking the user information\n",
        "user_info = !huggingface-cli whoami\n",
        "print(f\"Logged in as: {user_info[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BONV4GelxZla",
      "metadata": {
        "id": "BONV4GelxZla"
      },
      "source": [
        "> **âš ï¸ Important Notice:**\n",
        ">\n",
        "> Before you start the training, make sure to edit the `act_aloha_real.yaml` file located at:\n",
        ">\n",
        "> **Click Here** >> `/content/lerobot/lerobot/configs/policy/act_aloha_real.yaml`\n",
        ">\n",
        "> This file contains crucial parameters such as `batch_size`, `offline_steps`, and `learning_rate`. You should update these parameters based on your training needs. For example, you can modify:\n",
        ">\n",
        "> - **Batch Size** (`training.batch_size`): Adjust the number of samples processed in each training step.\n",
        "> - **Offline Training Steps** (`training.offline_steps`): Define how many steps to run during offline training.\n",
        "> - **Learning Rate** (`training.lr`): Set the learning rate to control how quickly the model learns.\n",
        ">\n",
        "> Once the file is updated, you can proceed with training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbac0593",
      "metadata": {
        "id": "dbac0593"
      },
      "source": [
        "## Step 5: Model Training or Resumption\n",
        "\n",
        "Now that everything is set up, we can either begin training the model or resume training from the last checkpoint, depending on your input.\n",
        "\n",
        "If resuming, make sure the checkpoint is available in your session. The training will continue from the last checkpoint if found.\n",
        "\n",
        "> **âš ï¸ Important: GPU Usage**\n",
        ">\n",
        "> By default, the training is configured to use a **GPU** for faster computation. If the runtime does not have access to a GPU, the training will fail.\n",
        ">\n",
        "> To avoid this issue:\n",
        ">\n",
        "> - **Ensure GPU is enabled** in your Colab runtime. You can check this by navigating to **Runtime > Change runtime type > Hardware accelerator** and selecting **GPU**.\n",
        "> - If you prefer to use a **CPU** instead, update the `device` argument to `device=cpu` in the training command in the next cell.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea8896f6",
      "metadata": {
        "id": "ea8896f6"
      },
      "outputs": [],
      "source": [
        "# Start or resume training depending on user choice\n",
        "%cd /content/lerobot/\n",
        "\n",
        "if resume_flag.lower() == \"no\":\n",
        "    print(f\"Starting new training on {dataset_repo_id}...\")\n",
        "    !python lerobot/scripts/train.py dataset_repo_id={dataset_repo_id} policy=act_aloha_real env=aloha_real hydra.run.dir=outputs/train/{output_dir} hydra.job.name={job_name} device=cuda wandb.enable=false\n",
        "else:\n",
        "    print(f\"Resuming training from {output_dir}... (ensure checkpoint is available)\")\n",
        "    !python lerobot/scripts/train.py hydra.run.dir={output_dir} resume=true\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79fe95cd",
      "metadata": {
        "id": "79fe95cd"
      },
      "source": [
        "\n",
        "## Step 5: Uploading the Model (Recommended)\n",
        "\n",
        "Once the model is trained, you can choose to upload it to Hugging Face for safekeeping. This is **highly recommended** if you are running long sessions or training a valuable model.\n",
        "\n",
        "Uploading the model will help protect against potential session interruptions or failures.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10eec178",
      "metadata": {
        "id": "10eec178"
      },
      "outputs": [],
      "source": [
        "print(model_repo_id)\n",
        "# Model upload step if chosen\n",
        "if upload_choice.lower() == \"yes\":\n",
        "    print(\"Uploading the model to Hugging Face...\")\n",
        "    !huggingface-cli upload {model_repo_id}  outputs/train/{output_dir}/checkpoints/last/pretrained_model\n",
        "    print(\"Model uploaded to Hugging Face successfully.\")\n",
        "else:\n",
        "    print(\"Model upload skipped.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "061e20bd",
      "metadata": {
        "id": "061e20bd"
      },
      "source": [
        "\n",
        "## Step 6: Safeguarding Session Data and Local Storage\n",
        "\n",
        "To prevent data loss in case of session termination, you can zip the output directory and download it locally. If you selected local storage, the outputs will be saved to your local machine.\n",
        "\n",
        "Make sure to run this step to save all training outputs before closing your session.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "327f72e0",
      "metadata": {
        "id": "327f72e0"
      },
      "outputs": [],
      "source": [
        "# Zip the output directory and download it if local storage is chosen\n",
        "%cd /content/lerobot/outputs/train/\n",
        "!ls\n",
        "if store_locally.lower() == \"yes\":\n",
        "    print(\"Zipping outputs for download...\")\n",
        "    !zip -r trained.zip {output_dir}\n",
        "\n",
        "    # Download the zipped file\n",
        "    from google.colab import files\n",
        "    files.download('/content/lerobot/outputs/train/trained.zip')\n",
        "else:\n",
        "    print(\"Local storage not selected, skipping download.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba7bbcc4",
      "metadata": {
        "id": "ba7bbcc4"
      },
      "source": [
        "\n",
        "# Troubleshooting and Recommendations\n",
        "\n",
        "1. **GPU Availability**: Ensure the selected GPU is available on your cloud platform (e.g., Colab).\n",
        "2. **Compute Units**: Ensure you have sufficient compute units. Each 5-hour session requires ~70 units.\n",
        "3. **Hugging Face Token**: You can generate a token [here](https://huggingface.co/settings/tokens).\n",
        "4. **Session Safeguards**: Always download your results (output files) to prevent data loss if the session terminates.\n",
        "5. **Checkpoint Reminder**: If resuming training, ensure that the checkpoint file from the previous session is present in the session.\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}